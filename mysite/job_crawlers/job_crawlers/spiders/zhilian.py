import scrapy
import json
import re
from datetime import datetime
from urllib.parse import urljoin, urlencode, quote
import os
import sys
import django

# Setup Django - add parent directory to path and configure Django
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../..'))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'local.settings.dev')
django.setup()

from wagtail.models import Page
from jobs.models import JobPage, JobIndexPage
from django.utils import timezone
from twisted.internet import threads, defer

class ZhilianSpider(scrapy.Spider):
    name = 'zhilian'
    allowed_domains = ['www.zhaopin.com', 'sou.zhaopin.com']
    
    # åŸå¸‚ä»£ç æ˜ å°„ï¼ˆåŒ—äº¬=530ï¼Œä¸Šæµ·=538ï¼Œæ·±åœ³=765ç­‰ï¼‰
    CITY_CODES = {
        'åŒ—äº¬': '530',
        'ä¸Šæµ·': '538',
        'æ·±åœ³': '765',
        'å¹¿å·': '763',
        'æ­å·': '653',
        'æˆéƒ½': '801',
    }
    
    def start_requests(self):
        keywords = ['Python', 'Java', 'å‰ç«¯', 'åç«¯', 'ç®—æ³•']
        city = 'å¹¿å·'  
        
        for keyword in keywords:
            # ä½¿ç”¨æ™ºè”æ‹›è˜çš„æœç´¢URL
            # æ–¹å¼1: ä½¿ç”¨æ–°çš„æœç´¢é¡µé¢
            url = f'https://sou.zhaopin.com/?jl={self.CITY_CODES.get(city, "530")}&kw={quote(keyword)}&p=1'
            
            yield scrapy.Request(
                url=url,
                callback=self.parse_list,
                meta={'keyword': keyword, 'city': city, 'page': 1},
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Referer': 'https://www.zhaopin.com/',
                },
                dont_filter=True
            )
    
    def parse_list(self, response):
        job_links = response.css('a[href*="/job_detail/"]::attr(href)').getall()
        
        # æ–¹å¼2: æŸ¥æ‰¾åŒ…å«èŒä½IDçš„é“¾æ¥
        if not job_links:
            job_links = response.css('a[href*="job"]::attr(href)').getall()
        
        # æ–¹å¼3: ä»é¡µé¢ä¸­æå–æ‰€æœ‰å¯èƒ½çš„èŒä½é“¾æ¥
        if not job_links:
            # å°è¯•ä»JavaScriptæ•°æ®ä¸­æå–
            script_text = response.text
            # æŸ¥æ‰¾åŒ…å«jobIdæˆ–positionIdçš„æ¨¡å¼
            job_ids = re.findall(r'job[Ii]d["\']?\s*[:=]\s*["\']?(\d+)', script_text)
            for job_id in job_ids:
                job_links.append(f'https://www.zhaopin.com/job_detail/{job_id}.html')
        
        if job_links:
            self.logger.debug(f"æ‰¾åˆ° {len(job_links)} ä¸ªèŒä½é“¾æ¥")
        
        for link in job_links:
            # ç¡®ä¿é“¾æ¥æ˜¯å®Œæ•´çš„URL
            if not link.startswith('http'):
                link = urljoin('https://www.zhaopin.com', link)
            
            # ç›´æ¥yieldè¯·æ±‚ï¼Œåœ¨parse_detailä¸­è¿›è¡Œå»é‡æ£€æŸ¥
            yield scrapy.Request(
                url=link,
                callback=self.parse_detail,
                meta={'source_url': link},
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Referer': response.url,
                },
                dont_filter=False
            )
        
        # ç¿»é¡µé€»è¾‘
        next_page = response.css('a.next-page::attr(href)').get()
        if not next_page:
            # å°è¯•å…¶ä»–ç¿»é¡µé€‰æ‹©å™¨
            next_page = response.css('a[class*="next"]::attr(href)').get()
        
        if next_page:
            if not next_page.startswith('http'):
                next_page = urljoin(response.url, next_page)
            
            page = response.meta.get('page', 1) + 1
            if page <= 10:  # é™åˆ¶æœ€å¤šçˆ¬å–10é¡µ
                yield scrapy.Request(
                    url=next_page,
                    callback=self.parse_list,
                    meta={'keyword': response.meta.get('keyword'), 'city': response.meta.get('city'), 'page': page},
                    headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Referer': response.url,
                    },
                    dont_filter=True
                )
    
    @defer.inlineCallbacks
    def parse_detail(self, response):
        """è§£æèŒä½è¯¦æƒ…é¡µé¢"""
        source_url = response.meta.get('source_url', response.url)
        
        # æ£€æŸ¥å“åº”çŠ¶æ€å’Œå†…å®¹ç±»å‹
        content_type = response.headers.get('Content-Type', b'').decode('utf-8', errors='ignore')
        content_encoding = response.headers.get('Content-Encoding', b'').decode('utf-8', errors='ignore')
        
        # æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«HTML
        try:
            response_text = response.text[:500] if len(response.text) > 500 else response.text
            has_html = '<html' in response_text.lower() or '<!DOCTYPE' in response_text.upper()
            has_title = '<title' in response_text.lower()
        except Exception as e:
            self.logger.error(f"æ— æ³•è§£ç å“åº”æ–‡æœ¬: {str(e)}")
            self.logger.error(f"å¯èƒ½æ˜¯å‹ç¼©æ ¼å¼é—®é¢˜ï¼ŒContent-Encoding: {content_encoding}")
            # å°è¯•æ‰‹åŠ¨è§£å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if 'br' in content_encoding.lower():
                self.logger.error("âš ï¸  å“åº”ä½¿ç”¨Brotliå‹ç¼©ï¼Œä½†æ— æ³•è§£å‹ï¼")
                self.logger.error("   è§£å†³æ–¹æ¡ˆï¼šå®‰è£… brotli æˆ– brotlicffi åº“")
                self.logger.error("   å‘½ä»¤ï¼špip install brotli æˆ– pip install brotlicffi")
            return []  # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯None
        
        self.logger.debug(f"å“åº”åŒ…å« <html>: {has_html}")
        self.logger.debug(f"å“åº”åŒ…å« <title>: {has_title}")
        
        # å¦‚æœå“åº”ä¸æ˜¯HTMLï¼Œè®°å½•è­¦å‘Š
        if not has_html and not has_title:
            self.logger.warning(f"âš ï¸  å“åº”å¯èƒ½ä¸æ˜¯HTMLæ ¼å¼ï¼")
            self.logger.warning(f"   Content-Encoding: {content_encoding}")
            self.logger.warning(f"   å“åº”å‰200å­—ç¬¦: {response_text[:200]}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯JSONå“åº”
            try:
                import json
                json_data = json.loads(response.text)
                self.logger.warning(f"   å“åº”æ˜¯JSONæ ¼å¼: {type(json_data)}")
                self.logger.warning(f"   JSONå†…å®¹é¢„è§ˆ: {str(json_data)[:200]}")
            except:
                pass
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯é¡µé¢
            if response.status >= 400:
                self.logger.error(f"   HTTPé”™è¯¯çŠ¶æ€ç : {response.status}")
                return []  # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯None
            
            # å¦‚æœæ˜¯å‹ç¼©é—®é¢˜
            if 'br' in content_encoding.lower() or 'brotli' in content_encoding.lower():
                self.logger.error("âš ï¸  å“åº”ä½¿ç”¨Brotliå‹ç¼©ä½†æ— æ³•è§£å‹ï¼")
                self.logger.error("   è¯·å®‰è£… brotli åº“: pip install brotli")
                return []  # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯None
        
        try:
            # æå–èŒä½ä¿¡æ¯ï¼ˆè¿™äº›æ“ä½œæ˜¯åŒæ­¥çš„ï¼Œä¸éœ€è¦åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
            # èŒä½æ ‡é¢˜ - æ ¹æ®å®é™…HTMLç»“æ„ï¼š<h1 class="summary-plane__title">èŒä½åç§°<img ...></h1>
            # éœ€è¦æ’é™¤imgæ ‡ç­¾ï¼Œåªæå–æ–‡æœ¬å†…å®¹
            job_title = None
            
            # æ–¹æ³•1ï¼šä½¿ç”¨XPathè·å–h1çš„ç›´æ¥æ–‡æœ¬å†…å®¹ï¼ˆè‡ªåŠ¨æ’é™¤imgç­‰å­å…ƒç´ ï¼‰
            h1_element = response.css('h1.summary-plane__title')
            if h1_element:
                # XPathçš„text()åªè·å–ç›´æ¥æ–‡æœ¬èŠ‚ç‚¹ï¼Œä¸åŒ…æ‹¬å­å…ƒç´ çš„æ–‡æœ¬
                job_title = h1_element.xpath('text()').get()
                if job_title:
                    job_title = job_title.strip()
            
            # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œå°è¯•CSSé€‰æ‹©å™¨
            if not job_title:
                title_selectors = [
                    'h1.summary-plane__title::text',  # ä¼˜å…ˆä½¿ç”¨å®é™…çš„ç»“æ„
                    '.summary-plane__title::text',
                    'h1.job-title::text',
                    'h1::text',
                    '.job-title::text',
                    '.position-title::text',
                    '[class*="job-title"]::text',
                    '[class*="position-title"]::text',
                    '[class*="summary-plane"] h1::text',
                    'title::text'
                ]
                for selector in title_selectors:
                    job_title = response.css(selector).get()
                    if job_title:
                        job_title = job_title.strip()
                        if job_title:
                            break
            
            # æ–¹æ³•3ï¼šå¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œä»h1å…ƒç´ HTMLä¸­æå–å¹¶æ¸…ç†æ ‡ç­¾
            if not job_title:
                h1_element = response.css('h1.summary-plane__title')
                if h1_element:
                    h1_html = h1_element.get()
                    if h1_html:
                        # ç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾ï¼ˆåŒ…æ‹¬imgï¼‰
                        job_title = re.sub(r'<[^>]+>', '', h1_html)
                        job_title = job_title.strip()
            
            # æ¸…ç†æ ‡é¢˜
            if job_title:
                job_title = job_title.strip()
                # æ¸…ç†æ ‡é¢˜ä¸­çš„ç½‘ç«™åç§°å’Œå¤šä½™ç©ºç™½
                job_title = re.sub(r'[-_]\s*æ™ºè”æ‹›è˜.*$', '', job_title, flags=re.IGNORECASE)
                job_title = re.sub(r'\s*-\s*æ™ºè”æ‹›è˜.*$', '', job_title, flags=re.IGNORECASE)
                job_title = re.sub(r'\s+', ' ', job_title).strip()  # æ¸…ç†å¤šä½™ç©ºç™½
            
            # å¦‚æœCSSé€‰æ‹©å™¨éƒ½å¤±è´¥ï¼Œå°è¯•ä»titleæ ‡ç­¾æå–
            if not job_title:
                title_text = response.css('title::text').get() or ''
                # ä»titleä¸­æå–èŒä½åç§°ï¼ˆé€šå¸¸åœ¨"-"ä¹‹å‰ï¼‰
                title_match = re.search(r'^([^-]+)', title_text)
                if title_match:
                    job_title = title_match.group(1).strip()
                    job_title = re.sub(r'[-_]\s*æ™ºè”æ‹›è˜.*$', '', job_title, flags=re.IGNORECASE)
            
            # å…¬å¸åç§° - æ ¹æ®å®é™…HTMLç»“æ„ï¼š
            # <li class="company-info">
            #   <strong class="company-info__title">å…¥èŒå…¬å¸:</strong>
            #   <span class="company-info__description">åŒ—äº¬æ™ºè°±åç« ç§‘æŠ€è‚¡ä»½æœ‰é™å…¬å¸</span>
            # </li>
            # æˆ–è€…ï¼š<a href="..." class="company__title">å…¬å¸åç§°</a>
            company_name = None
            
            # æ–¹æ³•1ï¼šä» join-company__content ä¸­æå–ï¼ˆä¼˜å…ˆï¼‰
            company_info = response.css('.join-company__content')
            if company_info:
                # æŸ¥æ‰¾åŒ…å«"å…¥èŒå…¬å¸"çš„li
                company_li = company_info.css('li.company-info')
                for li in company_li:
                    title = li.css('strong.company-info__title::text').get() or ''
                    if 'å…¥èŒå…¬å¸' in title or 'å…¬å¸' in title:
                        company_name = li.css('span.company-info__description::text').get()
                        if company_name:
                            company_name = company_name.strip()
                            break
            
            # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œå°è¯• company__title
            if not company_name:
                company_selectors = [
                    'a.company__title::text',  # ä¼˜å…ˆä½¿ç”¨å®é™…çš„ç»“æ„
                    '.company__title::text',  # å¤‡ç”¨ï¼šä¸æŒ‡å®šæ ‡ç­¾
                    'a.company__title',  # è·å–æ•´ä¸ªaå…ƒç´ 
                    '.summary-plane__company::text',  # å¤‡ç”¨ï¼šsummary-planeç»“æ„
                    '.summary-plane__company a::text',
                    'a[href*="/company/"]::text',
                    'a[href*="companydetail"]::text',  # åŒ¹é…å…¬å¸è¯¦æƒ…é¡µé“¾æ¥
                    '.company-name::text',
                    '.company-name a::text',
                    '[class*="company-name"]::text',
                    '[class*="company"] a::text',
                    '.job-company::text',
                    '.company::text',
                    '[class*="summary-plane"][class*="company"]::text'
                ]
                for selector in company_selectors:
                    company_name = response.css(selector).get()
                    if company_name:
                        company_name = company_name.strip()
                        # æ¸…ç†å¯èƒ½çš„HTMLæ ‡ç­¾æ®‹ç•™
                        company_name = re.sub(r'<[^>]+>', '', company_name).strip()
                        if company_name:
                            break
            
            # æ–¹æ³•3ï¼šå¦‚æœç›´æ¥è·å–æ–‡æœ¬å¤±è´¥ï¼Œå°è¯•è·å–aå…ƒç´ ç„¶åæå–æ–‡æœ¬
            if not company_name:
                company_element = response.css('a.company__title')
                if company_element:
                    company_name = ''.join(company_element.css('*::text').getall())
                    if not company_name:
                        company_name = company_element.get()
                        if company_name:
                            company_name = re.sub(r'<[^>]+>', '', company_name)
                    if company_name:
                        company_name = company_name.strip()
            
            # å¦‚æœCSSé€‰æ‹©å™¨éƒ½å¤±è´¥ï¼Œå°è¯•ä»é¡µé¢æ–‡æœ¬ä¸­æå–
            if not company_name:
                # æŸ¥æ‰¾åŒ…å«"å…¬å¸"çš„æ–‡æœ¬æ¨¡å¼
                page_text = response.text
                company_match = re.search(r'å…¥èŒå…¬å¸[ï¼š:]\s*([^\s<]+)', page_text)
                if not company_match:
                    company_match = re.search(r'å…¬å¸[åç§°]?[ï¼š:]\s*([^\s<]+)', page_text)
                if company_match:
                    company_name = company_match.group(1).strip()
            
            # å·¥ä½œåœ°ç‚¹ - æ ¹æ®å®é™…HTMLç»“æ„ï¼š
            # <ul class="summary-plane__info">
            #   <li><a href="//www.zhaopin.com/beijing/" target="_blank">åŒ—äº¬</a><span>æµ·æ·€åŒº</span></li>
            # </ul>
            # æˆ–è€…ï¼š<span class="job-address__content-text">åŒ—äº¬æµ·æ·€åŒºæœç‹ç½‘ç»œå¤§å¦9</span>
            location = None
            
            # æ–¹æ³•1ï¼šä» summary-plane__info çš„ç¬¬ä¸€ä¸ªliä¸­æå–ï¼ˆåŸå¸‚+åŒºåŸŸï¼‰
            info_list = response.css('ul.summary-plane__info')
            if info_list:
                first_li_list = info_list.css('li')
                if first_li_list:
                    first_li = first_li_list[0]  # ä½¿ç”¨ç´¢å¼•è·å–ç¬¬ä¸€ä¸ªå…ƒç´ 
                    # è·å–liå†…çš„æ‰€æœ‰æ–‡æœ¬ï¼ˆåŒ…æ‹¬aå’Œspanæ ‡ç­¾çš„æ–‡æœ¬ï¼‰
                    city_text = first_li.css('a::text').get() or ''
                    area_text = first_li.css('span::text').get() or ''
                    if city_text or area_text:
                        location = (city_text.strip() + area_text.strip()).strip()
            
            # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œå°è¯• job-address__content-text
            if not location:
                location_selectors = [
                    'span.job-address__content-text::text',  # ä¼˜å…ˆä½¿ç”¨å®é™…çš„ç»“æ„
                    '.job-address__content-text::text',  # å¤‡ç”¨ï¼šä¸æŒ‡å®šæ ‡ç­¾
                    'span.job-address__content-text',  # è·å–æ•´ä¸ªspanå…ƒç´ 
                    '.summary-plane__location::text',  # å¤‡ç”¨ï¼šsummary-planeç»“æ„
                    '.summary-plane__area::text',
                    '.summary-plane__city::text',
                    '.job-location::text',
                    '.workplace::text',
                    '.location::text',
                    '[class*="location"]::text',
                    '[class*="workplace"]::text',
                    '[class*="area"]::text',
                    '[class*="address"]::text',
                    '.job-area::text',
                    '[class*="summary-plane"][class*="location"]::text'
                ]
                for selector in location_selectors:
                    location = response.css(selector).get()
                    if location:
                        location = location.strip()
                        # æ¸…ç†å¯èƒ½çš„HTMLæ ‡ç­¾æ®‹ç•™å’Œå›¾æ ‡æ–‡æœ¬
                        location = re.sub(r'<[^>]+>', '', location).strip()
                        # ç§»é™¤å¯èƒ½çš„å›¾æ ‡å­—ç¬¦æˆ–ç©ºç™½
                        location = re.sub(r'^\s*[ğŸ“ğŸ”]\s*', '', location).strip()
                        if location:
                            break
            
            # æ–¹æ³•3ï¼šå¦‚æœç›´æ¥è·å–æ–‡æœ¬å¤±è´¥ï¼Œå°è¯•è·å–spanå…ƒç´ ç„¶åæå–æ–‡æœ¬ï¼ˆæ’é™¤å›¾æ ‡ï¼‰
            if not location:
                location_element = response.css('span.job-address__content-text')
                if location_element:
                    # è·å–æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹ï¼Œä½†æ’é™¤å›¾æ ‡å†…çš„æ–‡æœ¬
                    all_texts = location_element.css('*::text').getall()
                    # è¿‡æ»¤æ‰å¯èƒ½æ˜¯å›¾æ ‡çš„å†…å®¹ï¼ˆé€šå¸¸å¾ˆçŸ­æˆ–åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼‰
                    location = ''.join([t.strip() for t in all_texts if t.strip() and len(t.strip()) > 1])
                    if not location:
                        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œè·å–æ•´ä¸ªå…ƒç´ å†…å®¹å¹¶æ¸…ç†
                        location = location_element.get()
                        if location:
                            # ç§»é™¤HTMLæ ‡ç­¾
                            location = re.sub(r'<[^>]+>', '', location)
                            # ç§»é™¤å¯èƒ½çš„å›¾æ ‡å­—ç¬¦
                            location = re.sub(r'[ğŸ“ğŸ”]', '', location).strip()
                    if location:
                        location = location.strip()
            
            # å¦‚æœCSSé€‰æ‹©å™¨éƒ½å¤±è´¥ï¼Œå°è¯•ä»é¡µé¢æ–‡æœ¬ä¸­æå–
            if not location:
                page_text = response.text
                location_match = re.search(r'å·¥ä½œ[åœ°ç‚¹|åŸå¸‚][ï¼š:]\s*([^\s<]+)', page_text)
                if location_match:
                    location = location_match.group(1).strip()
            
            # è–ªèµ„ - æ ¹æ®å®é™…HTMLç»“æ„ï¼š<span class="summary-plane__salary">1.5-1.8ä¸‡</span>
            salary = None
            salary_selectors = [
                'span.summary-plane__salary::text',  # ä¼˜å…ˆä½¿ç”¨å®é™…çš„ç»“æ„
                '.summary-plane__salary::text',  # å¤‡ç”¨ï¼šä¸æŒ‡å®šæ ‡ç­¾
                'span.summary-plane__salary',  # è·å–æ•´ä¸ªspanå…ƒç´ 
                '.summary-plane__pay::text',
                '.salary::text',
                '.job-salary::text',
                '[class*="salary"]::text',
                '.pay::text',
                '.wage::text',
                '[class*="summary-plane"][class*="salary"]::text'
            ]
            for selector in salary_selectors:
                salary = response.css(selector).get()
                if salary:
                    salary = salary.strip()
                    # æ¸…ç†å¯èƒ½çš„HTMLæ ‡ç­¾æ®‹ç•™
                    salary = re.sub(r'<[^>]+>', '', salary).strip()
                    if salary:
                        break
            
            # å¦‚æœç›´æ¥è·å–æ–‡æœ¬å¤±è´¥ï¼Œå°è¯•è·å–spanå…ƒç´ ç„¶åæå–æ–‡æœ¬
            if not salary:
                salary_element = response.css('span.summary-plane__salary')
                if salary_element:
                    salary = ''.join(salary_element.css('*::text').getall())
                    if not salary:
                        salary = salary_element.get()
                        if salary:
                            salary = re.sub(r'<[^>]+>', '', salary)
                    if salary:
                        salary = salary.strip()
            
            # å¦‚æœCSSé€‰æ‹©å™¨éƒ½å¤±è´¥ï¼Œå°è¯•ä»é¡µé¢æ–‡æœ¬ä¸­æå–è–ªèµ„
            if not salary:
                page_text = response.text
                # æŸ¥æ‰¾è–ªèµ„æ¨¡å¼ï¼šå¦‚"8-12K"ã€"10K-15K"ã€"é¢è®®"ç­‰
                salary_match = re.search(r'(è–ªèµ„|å·¥èµ„|å¾…é‡)[ï¼š:]\s*([^\s<]+)', page_text)
                if salary_match:
                    salary = salary_match.group(2).strip()
            
            # å¦‚æœå…³é”®å­—æ®µç¼ºå¤±ï¼Œå°è¯•ä»é¡µé¢ä¸­çš„JSONæ•°æ®æå–
            if (not job_title or not company_name) and '<script' in response.text:
                try:
                    # æŸ¥æ‰¾é¡µé¢ä¸­çš„JSONæ•°æ®
                    json_matches = re.findall(r'<script[^>]*type=["\']application/json["\'][^>]*>(.*?)</script>', response.text, re.DOTALL)
                    for json_str in json_matches:
                        try:
                            data = json.loads(json_str)
                            # é€’å½’æœç´¢JSONä¸­çš„èŒä½ä¿¡æ¯
                            def find_in_dict(d, keys):
                                if isinstance(d, dict):
                                    for k, v in d.items():
                                        if any(key in k.lower() for key in keys):
                                            if isinstance(v, str) and v:
                                                return v
                                        if isinstance(v, (dict, list)):
                                            result = find_in_dict(v, keys)
                                            if result:
                                                return result
                                elif isinstance(d, list):
                                    for item in d:
                                        result = find_in_dict(item, keys)
                                        if result:
                                            return result
                                return None
                            
                            if not job_title:
                                job_title = find_in_dict(data, ['title', 'jobname', 'position', 'name'])
                            if not company_name:
                                company_name = find_in_dict(data, ['company', 'companyname', 'employer'])
                            if not location:
                                location = find_in_dict(data, ['location', 'city', 'workplace', 'area'])
                            if not salary:
                                salary = find_in_dict(data, ['salary', 'pay', 'wage'])
                            
                            if job_title and company_name:
                                break
                        except json.JSONDecodeError:
                            continue
                except Exception:
                    pass
            
            # èŒä½æè¿° - æ ¹æ®å®é™…HTMLç»“æ„ï¼š
            # <div class="describtion__detail-content">
            #   <div>èŒä½æè¿°</div>
            #   <div>1ã€è´Ÿè´£ä¸å®¢æˆ·æ²Ÿé€š...</div>
            #   <div>èŒä½è¦æ±‚</div>
            #   <div>1ã€æœ¬ç§‘åŠä»¥ä¸Šå­¦å†...</div>
            # </div>
            description = ''
            
            # æ–¹æ³•1ï¼šä» describtion__detail-content ä¸­æå–ï¼ˆä¼˜å…ˆï¼‰
            detail_content = response.css('.describtion__detail-content')
            if detail_content:
                # è·å–æ‰€æœ‰divå†…çš„æ–‡æœ¬å†…å®¹
                desc_divs = detail_content.css('div')
                desc_parts = []
                for div in desc_divs:
                    div_text = ''.join(div.css('*::text').getall())
                    if not div_text:
                        div_html = div.get()
                        if div_html:
                            div_text = re.sub(r'<[^>]+>', ' ', div_html)
                    if div_text:
                        div_text = re.sub(r'\s+', ' ', div_text).strip()
                        # è·³è¿‡æ ‡é¢˜è¡Œï¼ˆå¦‚"èŒä½æè¿°"ã€"èŒä½è¦æ±‚"ï¼‰
                        if div_text and div_text not in ['èŒä½æè¿°', 'èŒä½è¦æ±‚', 'å²—ä½èŒè´£', 'å·¥ä½œå†…å®¹']:
                            desc_parts.append(div_text)
                
                if desc_parts:
                    description = '\n'.join(desc_parts)
            
            # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
            if not description:
                desc_selectors = [
                    '.describtion__detail-content',  # ä½¿ç”¨å®é™…çš„ç»“æ„
                    '.job-description',  # å¸¸è§çš„èŒä½æè¿°ç±»
                    '.position-detail',
                    '.job-detail',
                    '.job-des',
                    '.description',
                    '[class*="description"]',
                    '[class*="detail"]',
                    '[class*="job-desc"]',
                    '.position-content',
                    '.job-content',
                    '[class*="summary-plane"][class*="description"]',  # å¯èƒ½åœ¨summary-planeä¸­
                    '.detail-content',
                    '.job-intro'
                ]
                for selector in desc_selectors:
                    desc_elements = response.css(selector)
                    if desc_elements:
                        # å°è¯•è·å–æ‰€æœ‰æ–‡æœ¬å†…å®¹
                        desc_texts = desc_elements.css('*::text').getall()
                        if desc_texts:
                            description = ' '.join([t.strip() for t in desc_texts if t.strip()])
                            if description and len(description) > 20:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å®¹
                                break
            
            # æ–¹æ³•3ï¼šå¦‚æœCSSé€‰æ‹©å™¨éƒ½å¤±è´¥ï¼Œå°è¯•ä»é¡µé¢HTMLä¸­æå–
            if not description:
                page_text = response.text
                # æŸ¥æ‰¾åŒ…å«"èŒä½æè¿°"ã€"å²—ä½èŒè´£"ã€"å·¥ä½œå†…å®¹"çš„éƒ¨åˆ†
                desc_patterns = [
                    r'<div[^>]*class[^>]*describtion__detail-content[^>]*>(.*?)</div>',  # ä¼˜å…ˆåŒ¹é…å®é™…ç»“æ„
                    r'(èŒä½æè¿°|å²—ä½èŒè´£|å·¥ä½œå†…å®¹|å²—ä½è¦æ±‚)[ï¼š:]\s*([^<]*?)(ä»»èŒè¦æ±‚|å…¬å¸ä»‹ç»|èŒä½è¦æ±‚|$)',
                    r'<div[^>]*class[^>]*description[^>]*>(.*?)</div>',
                    r'<div[^>]*class[^>]*detail[^>]*>(.*?)</div>',
                ]
                for pattern in desc_patterns:
                    desc_match = re.search(pattern, page_text, re.DOTALL | re.IGNORECASE)
                    if desc_match:
                        desc_text = desc_match.group(1) if desc_match.lastindex >= 1 else desc_match.group(0)
                        # æ¸…ç†HTMLæ ‡ç­¾
                        desc_text = re.sub(r'<[^>]+>', ' ', desc_text)
                        desc_text = re.sub(r'\s+', ' ', desc_text).strip()
                        if desc_text and len(desc_text) > 20:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å®¹
                            description = desc_text
                            break
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æè¿°ï¼Œè‡³å°‘è®¾ç½®ä¸€ä¸ªé»˜è®¤å€¼
            if not description:
                description = 'æš‚æ— è¯¦ç»†æè¿°'
            
            # èŒä½ç±»å‹ï¼ˆå…¨èŒ/å®ä¹ /å…¼èŒï¼‰
            job_type = 'fulltime'  # é»˜è®¤å…¨èŒ
            type_text = response.text.lower()
            if 'å®ä¹ ' in type_text or 'intern' in type_text:
                job_type = 'intern'
            elif 'å…¼èŒ' in type_text or 'parttime' in type_text:
                job_type = 'parttime'
            
            # å‘å¸ƒæ—¶é—´
            publish_date = timezone.now()
            date_match = re.search(r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})', response.text)
            if date_match:
                try:
                    publish_date = datetime.strptime(date_match.group(1).replace('/', '-'), '%Y-%m-%d')
                except:
                    pass
            
            # éªŒè¯å¿…è¦å­—æ®µ
            missing_fields = []
            if not job_title:
                missing_fields.append("èŒä½æ ‡é¢˜")
            if not company_name:
                missing_fields.append("å…¬å¸åç§°")
            
            if missing_fields:
                self.logger.warning(f"ç¼ºå°‘å¿…è¦å­—æ®µï¼Œè·³è¿‡ä¿å­˜: {', '.join(missing_fields)} | URL: {response.url}")
                return []  # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯None
            
            # ä½¿ç”¨deferToThreadåœ¨çº¿ç¨‹ä¸­æ‰§è¡Œæ•°æ®åº“æ“ä½œï¼ˆåŒ…æ‹¬æ£€æŸ¥å’Œä¿å­˜ï¼‰
            yield threads.deferToThread(
                self._process_and_save_job,
                company_name=company_name,
                job_title=job_title,
                location=location or 'æœªçŸ¥',
                salary=salary or '',
                description=description or 'æš‚æ— è¯¦ç»†æè¿°',
                job_type=job_type,
                source_url=source_url,
                publish_date=publish_date
            )
            # è¿”å›ç©ºåˆ—è¡¨ï¼Œè¡¨ç¤ºæ²¡æœ‰æ–°çš„è¯·æ±‚éœ€è¦å¤„ç†
            return []
        
        except Exception as e:
            self.logger.error(f"è§£æèŒä½è¯¦æƒ…å¤±è´¥: {response.url}, é”™è¯¯: {str(e)}")
            import traceback
            self.logger.error(traceback.format_exc())
            # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯Noneï¼Œé¿å…è¿­ä»£é”™è¯¯
            return []
    
    def job_exists_sync(self, source_url):
        """åŒæ­¥ç‰ˆæœ¬çš„èŒä½å­˜åœ¨æ£€æŸ¥ï¼ˆç”¨äºåœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨ï¼‰"""
        if not source_url:
            return False
        
        # æå–èŒä½IDï¼ˆå¦‚æœURLä¸­åŒ…å«ï¼‰
        job_id_match = re.search(r'/(\d+)\.html', source_url)
        if job_id_match:
            job_id = job_id_match.group(1)
            return JobPage.objects.filter(source_url__contains=job_id).exists()
        
        # å¦åˆ™æ£€æŸ¥å®Œæ•´URL
        return JobPage.objects.filter(source_url=source_url).exists()
    
    def _process_and_save_job(self, company_name, job_title, location, salary, 
                               description, job_type, source_url, publish_date):
        """åœ¨çº¿ç¨‹ä¸­å¤„ç†å¹¶ä¿å­˜èŒä½ï¼ˆåŒ…æ‹¬å»é‡æ£€æŸ¥å’Œä¿å­˜ï¼‰"""
        from django.db import transaction, IntegrityError
        
        try:
            # ä½¿ç”¨æ•°æ®åº“äº‹åŠ¡ç¡®ä¿åŸå­æ€§æ“ä½œï¼Œé˜²æ­¢å¹¶å‘é‡å¤ä¿å­˜
            with transaction.atomic():
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆåœ¨äº‹åŠ¡ä¸­æ£€æŸ¥ï¼Œé˜²æ­¢å¹¶å‘é—®é¢˜ï¼‰
                existing_job = JobPage.objects.filter(source_url=source_url).first()
                if existing_job:
                    self.logger.debug(f"èŒä½å·²å­˜åœ¨ï¼Œè·³è¿‡: {job_title} - {company_name}")
                    return
                
                # è·å–æˆ–åˆ›å»ºçˆ¶é¡µé¢
                parent_page = JobIndexPage.objects.filter(slug='zhilian-jobs').first()
                
                if not parent_page:
                    # å°è¯•æŸ¥æ‰¾æ ¹é¡µé¢
                    try:
                        root_page = Page.objects.filter(depth=1).first()
                        if root_page:
                            # åˆ›å»ºJobIndexPage
                            parent_page = JobIndexPage(
                                title='æ™ºè”æ‹›è˜èŒä½',
                                slug='zhilian-jobs',
                                intro='æ¥è‡ªæ™ºè”æ‹›è˜çš„èŒä½ä¿¡æ¯'
                            )
                            root_page.add_child(instance=parent_page)
                            parent_page.save_revision().publish()
                            self.logger.debug("åˆ›å»ºäº†æ–°çš„èŒä½ç´¢å¼•é¡µ")
                    except Exception as e:
                        self.logger.warning(f"æ— æ³•åˆ›å»ºçˆ¶é¡µé¢ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤é¡µé¢: {str(e)}")
                        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„é¡µé¢
                        parent_page = Page.objects.filter(depth__gt=1).first()
                
                if not parent_page:
                    self.logger.error("æ— æ³•æ‰¾åˆ°æˆ–åˆ›å»ºçˆ¶é¡µé¢ï¼Œè·³è¿‡ä¿å­˜")
                    return
                
                # åˆ›å»ºJobPageå®ä¾‹
                job_page = JobPage(
                    title=f"{company_name}-{job_title}",
                    job_title=job_title,
                    company_name=company_name,
                    location=location,
                    salary=salary,
                    description=description,
                    job_type=job_type,
                    source_website='æ™ºè”æ‹›è˜',
                    source_url=source_url,
                    first_published_at=publish_date
                )
                
                # æ·»åŠ åˆ°é¡µé¢æ ‘å¹¶å‘å¸ƒï¼ˆåœ¨äº‹åŠ¡ä¸­æ‰§è¡Œï¼‰
                parent_page.add_child(instance=job_page)
                job_page.save_revision().publish()
                self.logger.info(f"âœ“ å·²ä¿å­˜: {job_title} - {company_name} ({location})")
            
        except IntegrityError:
            # æ•°æ®åº“å”¯ä¸€çº¦æŸå†²çªï¼ˆå¦‚æœè®¾ç½®äº†å”¯ä¸€çº¦æŸï¼‰
            self.logger.debug(f"èŒä½å¯èƒ½å·²å­˜åœ¨ï¼ˆæ•°æ®åº“çº¦æŸå†²çªï¼‰: {job_title} - {company_name}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜èŒä½å¤±è´¥: {job_title} - {company_name}, é”™è¯¯: {str(e)}")
            import traceback
            self.logger.error(traceback.format_exc())
