# 智联招聘爬虫运行指南

## 前置要求

### 1. 安装Python依赖

确保已安装以下Python包：

```bash
# 安装Scrapy（如果尚未安装）
pip install scrapy

# 或者安装所有项目依赖（包括Django、Wagtail等）
cd mysite
pip install -r requirements.txt
pip install scrapy
```

### 2. 确保MySQL数据库运行

爬虫会将数据保存到MySQL数据库，请确保：
- MySQL服务正在运行
- 数据库 `project` 已创建
- 数据库用户和密码正确（在 `local/settings/base.py` 中配置）

### 3. 确保Django迁移已完成

在运行爬虫之前，确保Django数据库迁移已完成：

```bash
cd mysite
python manage.py migrate
```

## 运行爬虫

### 方法1：在爬虫目录下运行（推荐）

```bash
# 进入爬虫目录
cd mysite/job_crawlers

# 运行爬虫
scrapy crawl zhilian
```

### 方法2：从项目根目录运行

```bash
# 从项目根目录运行
cd mysite
scrapy crawl zhilian -s JOBCRAWLERS_SETTINGS_MODULE=job_crawlers.settings
```

### 方法3：使用Python直接运行（用于调试）

```bash
cd mysite
python -m scrapy crawl zhilian
```

## 爬虫配置说明

### 修改搜索关键词和城市

编辑 `job_crawlers/job_crawlers/spiders/zhilian.py` 文件中的 `start_requests` 方法：

```python
def start_requests(self):
    # 修改这里的关键词和城市
    keywords = ['Python', 'Java', '前端', '后端', '算法']  # 搜索关键词
    city = '北京'  # 城市名称（支持：北京、上海、深圳、广州、杭州、成都）
    
    # ... 其余代码
```

### 调整爬取页数

在 `parse_list` 方法中修改页数限制：

```python
if page <= 10:  # 修改这里的数字，例如改为5表示只爬5页
```

### 调整爬取速度

编辑 `job_crawlers/job_crawlers/settings.py`：

```python
DOWNLOAD_DELAY = 2  # 增加这个值可以降低爬取速度，避免被封IP
CONCURRENT_REQUESTS_PER_DOMAIN = 2  # 减少并发请求数
```

## 查看爬取结果

### 1. 在Django管理后台查看

```bash
# 启动Django开发服务器
cd mysite
python manage.py runserver

# 访问 http://localhost:8000/admin/
# 登录后可以在Wagtail后台查看爬取的职位
```

### 2. 在数据库中直接查看

```sql
-- 连接到MySQL数据库
mysql -u root -p project

-- 查看爬取的职位
SELECT title, job_title, company_name, location, salary, source_url 
FROM jobs_jobpage 
ORDER BY first_published_at DESC 
LIMIT 20;
```

### 3. 查看爬虫日志

爬虫运行时会输出详细的日志信息，包括：
- 正在解析的URL
- 找到的职位数量
- 成功保存的职位
- 错误信息

## 常见问题

### 1. 导入错误：找不到Django模块

**错误信息：**
```
ModuleNotFoundError: No module named 'local'
```

**解决方法：**
确保在正确的目录下运行，并且Django设置模块路径正确：
```bash
cd mysite/job_crawlers
scrapy crawl zhilian
```

### 2. 数据库连接错误

**错误信息：**
```
django.db.utils.OperationalError: (2003, "Can't connect to MySQL server")
```

**解决方法：**
- 检查MySQL服务是否运行
- 检查数据库配置（`local/settings/base.py`）
- 确认数据库 `project` 已创建

### 3. 找不到父页面错误

**错误信息：**
```
Page matching query does not exist
```

**解决方法：**
在Wagtail后台创建一个根页面，或者修改爬虫代码中的父页面查找逻辑。

### 4. 爬取不到数据

**可能原因：**
- 网站结构发生变化，需要更新CSS选择器
- 网站使用了JavaScript动态加载，需要使用Splash或Selenium
- 被反爬虫机制拦截（IP被封、需要验证码等）

**解决方法：**
- 检查网站HTML结构是否变化
- 增加请求延迟时间
- 使用代理IP
- 考虑使用Splash渲染JavaScript内容

### 5. 爬虫运行缓慢

**解决方法：**
- 减少并发请求数：`CONCURRENT_REQUESTS_PER_DOMAIN = 1`
- 增加下载延迟：`DOWNLOAD_DELAY = 3`
- 减少爬取页数

## 高级用法

### 保存爬取数据到JSON文件

```bash
scrapy crawl zhilian -o jobs.json
```

### 保存爬取数据到CSV文件

```bash
scrapy crawl zhilian -o jobs.csv
```

### 启用详细日志

```bash
scrapy crawl zhilian -L INFO  # 或 -L DEBUG 获取更详细的日志
```

### 只爬取特定关键词

修改爬虫代码后运行，或者使用环境变量（需要修改代码支持）。

## 注意事项

1. **遵守网站使用条款**：请合理使用爬虫，不要对服务器造成过大压力
2. **遵守robots.txt**：虽然代码中设置了 `ROBOTSTXT_OBEY = False`，但请尊重网站的爬取规则
3. **数据去重**：爬虫会自动检查重复职位，基于URL和职位ID
4. **错误处理**：爬虫包含错误处理机制，单个职位失败不会影响整体运行
5. **数据验证**：爬取的数据会自动保存到MySQL数据库的 `JobPage` 模型中

## 技术支持

如果遇到问题，请检查：
1. Scrapy版本：`scrapy version`
2. Python版本：`python --version`
3. Django版本：`python -c "import django; print(django.get_version())"`
4. 查看完整错误日志
